<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: './public/search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="0.摘要本文主要介绍了：  卷积层的正向传播、反向传播(基于$img2col$方法) 转置卷积的概念和使用方法 常用的上采样方法(最邻近、双插值、转置卷积)及其优劣对比">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积、转置卷积与上采样">
<meta property="og:url" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/index.html">
<meta property="og:site_name" content="yerfor&#39;s Journey">
<meta property="og:description" content="0.摘要本文主要介绍了：  卷积层的正向传播、反向传播(基于$img2col$方法) 转置卷积的概念和使用方法 常用的上采样方法(最邻近、双插值、转置卷积)及其优劣对比">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/conv_signal_view.png">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/conv_img.gif">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/img2col.png">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/img2col-2.png">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/strides=2.gif">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/nearest.png">
<meta property="og:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/bilinear.png">
<meta property="article:published_time" content="2020-02-25T15:03:45.000Z">
<meta property="article:modified_time" content="2020-02-25T15:24:18.521Z">
<meta property="article:author" content="Zhenhui Ye">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="MachineLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/02/25/conv_deconv_upsample/conv_signal_view.png">

<link rel="canonical" href="http://yoursite.com/2020/02/25/conv_deconv_upsample/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>卷积、转置卷积与上采样 | yerfor's Journey</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yerfor's Journey</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/25/conv_deconv_upsample/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/face.jpg">
      <meta itemprop="name" content="Zhenhui Ye">
      <meta itemprop="description" content="Be a superhero.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yerfor's Journey">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          卷积、转置卷积与上采样
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-02-25 23:03:45 / 修改时间：23:24:18" itemprop="dateCreated datePublished" datetime="2020-02-25T23:03:45+08:00">2020-02-25</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0.摘要"></a>0.摘要</h1><p>本文主要介绍了：</p>
<ul>
<li>卷积层的正向传播、反向传播(基于$img2col$方法)</li>
<li>转置卷积的概念和使用方法</li>
<li>常用的上采样方法(最邻近、双插值、转置卷积)及其优劣对比</li>
</ul>
<a id="more"></a>
<h1 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1.卷积"></a>1.卷积</h1><h2 id="1-1卷积层的正向传播"><a href="#1-1卷积层的正向传播" class="headerlink" title="1.1卷积层的正向传播"></a>1.1卷积层的正向传播</h2><h3 id="1-1-1-常规思路理解卷积"><a href="#1-1-1-常规思路理解卷积" class="headerlink" title="1.1.1 常规思路理解卷积"></a>1.1.1 常规思路理解卷积</h3><p>参考资料：</p>
<p><a href="https://blog.csdn.net/fate_fjh/article/details/52882134" target="_blank" rel="noopener">CSDN-《卷积神经网络CNN——图像卷积与反卷积（后卷积，转置卷积）》</a></p>
<hr>
<p>《信号与系统》中卷积操作的定义如下：</p>
<script type="math/tex; mode=display">
[f*g][n]=\sum_{m=-M}^{M}f[n-m]\cdot g[m]</script><p>卷积的作用就是评价一个信号$g[m]$与一个信号模版$f[m]$的匹配程度，下面给一个直观的示例。</p>
<p>如下图所示，蓝色方波为待测信号$g[m]$，红色方波为信号模版$f[m]$，$m$是信号的带宽，如下图中蓝色方波就有$m=0.5$。卷积操作就是把信号模版函数沿y轴翻转，然后沿着x轴平移，记录下沿途$\sum_{m=-M}^{M}f[n-m]\cdot g[m]$的值，就得到了下图中三角形的函数，可以看到当$n=0$时，两个方波恰好重合，匹配度最高，因此卷积函数的值也最高。</p>
<p><img src="/2020/02/25/conv_deconv_upsample/conv_signal_view.png" alt="conv_signal_view"></p>
<p>在图像领域，卷积同样用来衡量两个信号的匹配程度。准确的说，图片对应上面例子中的待匹配信号$f$，卷积核对应信号模版$g$。一个卷积核对应一种特征，所以对图像做一次卷积就是判断图像中该卷积核对应特征的分布。图像卷积的运算过程如下图所示：</p>
<p><img src="/2020/02/25/conv_deconv_upsample/conv_img.gif" alt="conv_img"></p>
<p>一个卷积核可以描述<strong>边缘</strong>这样的低级特征，如Sobel算子等，对原图像做卷积运算得到的特征图就是边缘的分布图。传统图像处理领域里的卷积核都是人为给出的，而在深度学习中卷积核是训练出来的。</p>
<p>正如多个不同方向、长短的线段的组合可以描述出眼睛、嘴巴的形状，多个卷积层的堆叠可以提取出复杂的、高阶的特征，深度卷积神经网络就是利用了这点，传统的CNN用卷积层提取特征，用池化层进行特征降维，然后用全连接层对提取到的高阶特征进行关联。2014年提出的FCN(Fully Convolutional Network)则发现不使用全连接层，只用卷积层达到了更好的效果。</p>
<p>说句题外话，以下只是我的猜测：Google的InceptionNet将3x3的卷积层拆分成一个1x3和3x1的卷积层，这样减少了计算量，从而提升了速度；但我觉得这样虽然保证了相同的感受野，但是可以拟合的特征的形状可能相比3x3的卷积核会少一些(?)，因此它的性能要稍微差一点。</p>
<h3 id="1-1-2-用矩阵乘法计算卷积（im2col法实现）"><a href="#1-1-2-用矩阵乘法计算卷积（im2col法实现）" class="headerlink" title="1.1.2 用矩阵乘法计算卷积（im2col法实现）"></a>1.1.2 用矩阵乘法计算卷积（im2col法实现）</h3><p>参考资料：</p>
<p><a href="https://zhuanlan.zhihu.com/p/40951745" target="_blank" rel="noopener">知乎-《反向传播之六：CNN 卷积层反向传播》</a></p>
<p><a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/" target="_blank" rel="noopener">《Why GEMM is at the heart of deep learning》</a></p>
<hr>
<ul>
<li>卷积计算通常的两种实现方式是：在caffe中使用im2col的方法，在其他的地方使用toeplitz matrix(托普利兹矩阵)进行实现。本文介绍的是im2col方法。</li>
</ul>
<p>在上一小节用一张动图描述了图像卷积的正向计算过程，但这种直观的做法最大的问题是无法并行化，时间复杂度是很高的，现有的深度学习框架基本都把卷积计算转化成矩阵乘法来提升效率。</p>
<p>下面用一个简单的例子介绍如何将卷积计算转化成矩阵：</p>
<p>假设现有单通道、尺寸为$[3,3]$的输入张量$X$，一个$kernel_{-}size=(2,2)$的卷积核$K$，做$strides=1$，$padding=valid$的卷积计算。</p>
<script type="math/tex; mode=display">
X =\left[
    \begin{matrix}
    x_{11} & x_{12} & x_{13}\\
    x_{21} & x_{22} & x_{23}\\
    x_{31} & x_{32} & x_{33}
    \end{matrix}
\right]
,
K = 
\left[
    \begin{matrix}
    k_{11} & k_{12}\\
    k_{21} & k_{22}
    \end{matrix}
\right]
,
Y=
\left[
    \begin{matrix}
    y_{11} & y_{12}\\
    y_{21} & y_{22}
    \end{matrix}
\right]</script><p>那么就有：</p>
<script type="math/tex; mode=display">
\left[
    \begin{matrix}
    x_{11} & x_{12} & x_{13}\\
    x_{21} & x_{22} & x_{23}\\
    x_{31} & x_{32} & x_{33}
    \end{matrix}
\right]
*
\left[
    \begin{matrix}
    k_{11} & k_{12}\\
    k_{21} & k_{22}
    \end{matrix}
\right]
=
\left[
    \begin{matrix}
    y_{11} & y_{12}\\
    y_{21} & y_{22}
    \end{matrix}
\right]</script><p>如何用矩阵计算卷积操作呢？考虑直观的卷积计算过程，在本示例中要进行<code>左上、右上、左下、右下</code>共四次计算，我们将卷积核矩阵$K$和输出矩阵$Y$变形成$[4,1]$，每一行用来处理上述四次计算中的一次，即有：</p>
<script type="math/tex; mode=display">
\left[
    \begin{matrix}
    y_{11}\\
    y_{12}\\
    y_{21}\\
    y_{22}\\
    \end{matrix}
\right]
=
\left[
    \begin{matrix}
    x_{11}k_{11} + x_{12}k_{12} + x_{21}k_{21} + x_{22}k_{22}\\
    x_{12}k_{12} + x_{13}k_{13} + x_{22}k_{22} + x_{23}k_{23}\\
    x_{21}k_{21} + x_{22}k_{22} + x_{31}k_{31} + x_{32}k_{32}\\
    x_{22}k_{22} + x_{23}k_{23} + x_{32}k_{32} + x_{33}k_{33}\\
    \end{matrix}
\right]
\\
=
\left[
    \begin{matrix}
    x_{11} & x_{12} & x_{21} & x_{22}\\
    x_{12} & x_{13} & x_{22} & x_{23}\\
    x_{21} & x_{22} & x_{31} & x_{32}\\
    x_{22} & x_{23} & x_{32} & x_{33}\\
    \end{matrix}
\right]
\cdot
\left[
    \begin{matrix}
    k_{11}\\
    k_{12}\\
    k_{21}\\
    k_{22}\\
    \end{matrix}
\right]</script><p>通过上述方法，将卷积转化成了矩阵计算，这里对各个矩阵定义如下：</p>
<script type="math/tex; mode=display">
Y_c=\left[
    \begin{matrix}
    y_{11}\\
    y_{12}\\
    y_{21}\\
    y_{22}\\
    \end{matrix}
\right]
,
X_c=
\left[
    \begin{matrix}
    x_{11} & x_{12} & x_{21} & x_{22}\\
    x_{12} & x_{13} & x_{22} & x_{23}\\
    x_{21} & x_{22} & x_{31} & x_{32}\\
    x_{22} & x_{23} & x_{32} & x_{33}\\
    \end{matrix}
\right]
,
K_c=
\left[
    \begin{matrix}
    k_{11}\\
    k_{12}\\
    k_{21}\\
    k_{22}\\
    \end{matrix}
\right]</script><p>其中，$Y_c$和$K_c$都是简单的对初识矩阵进行一个$reshape$即可得到，实现$X\rightarrow X_c$的转化则比较复杂，我们用一个函数$img2col$来实现这个过程，核心思路是$X_c$矩阵的每一行是该次计算时卷积核在$X$上的感受野。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">im2col</span><span class="params">(img, kernel_size, strides)</span>:</span></span><br><span class="line">    <span class="comment"># img is a 4d tensor([batch_size, width ,height, channel])</span></span><br><span class="line">    img_col = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, img.shape[<span class="number">1</span>] - kernel_size + <span class="number">1</span>, strides):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, image.shape[<span class="number">2</span>] - kernel_size + <span class="number">1</span>, strides):</span><br><span class="line">          <span class="comment"># 遍历了所有的中心点,分别读取其感受野，然后reshape成列向量</span></span><br><span class="line">            col = img[:, i:i + kernel_size, j:j + kernel_size, :].reshape([<span class="number">-1</span>])</span><br><span class="line">            img_col.append(col)</span><br><span class="line">    img_col = np.array(img_col)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image_col</span><br></pre></td></tr></table></figure>
<p>下图有助于对$img2col$过程的理解：</p>
<p><img src="/2020/02/25/conv_deconv_upsample/img2col.png" alt="img2col"></p>
<p><img src="/2020/02/25/conv_deconv_upsample/img2col-2.png" alt="img2col"></p>
<h2 id="1-2-卷积层的反向传播"><a href="#1-2-卷积层的反向传播" class="headerlink" title="1.2 卷积层的反向传播"></a>1.2 卷积层的反向传播</h2><h3 id="1-2-1-计算权重-K-和偏置-B-的梯度"><a href="#1-2-1-计算权重-K-和偏置-B-的梯度" class="headerlink" title="1.2.1 计算权重$K$和偏置$B$的梯度"></a>1.2.1 计算权重$K$和偏置$B$的梯度</h3><p>参考资料：</p>
<p><a href="https://zhuanlan.zhihu.com/p/40951745" target="_blank" rel="noopener">知乎-《反向传播之六：CNN 卷积层反向传播》</a></p>
<hr>
<p>卷积层的参数有卷积核权重$K$和偏置$B$，参数的更新用梯度下降法实现：</p>
<script type="math/tex; mode=display">
K \leftarrow K - \eta \cdot \frac{\partial Loss}{\partial K},B \leftarrow B - \eta \cdot \frac{\partial Loss}{\partial B}</script><p>为了方便起见，定义$Loss$的偏导数的简写方法：$\nabla =\frac{\partial Loss}{\partial }$。</p>
<p>根据BP算法的性质，考虑一个卷积层的反向传播时，我们已经知道其输出矩阵$Y$的梯度$\nabla Y$：</p>
<script type="math/tex; mode=display">
\nabla Y =
\left[
    \begin{matrix}
    \nabla y_{11} & \nabla y_{12}\\
    \nabla y_{21} & \nabla y_{22}\\
    \end{matrix}
\right
]</script><p>将其做矩阵变形，得到<code>1.1.2</code>小节所述的$\nabla Y_c$矩阵：</p>
<script type="math/tex; mode=display">
\nabla Y_c =
\left[
    \begin{matrix}
    \nabla y_{11} \\
  \nabla y_{12} \\
    \nabla y_{21} \\
  \nabla y_{22} \\
    \end{matrix}
\right
]</script><h4 id="1-2-1-1-偏置B的梯度"><a href="#1-2-1-1-偏置B的梯度" class="headerlink" title="1.2.1.1 偏置B的梯度"></a>1.2.1.1 偏置B的梯度</h4><p>首先考虑偏执$B$的梯度，由链式法则有：</p>
<script type="math/tex; mode=display">
\nabla B = \nabla Y_c \cdot \frac{\partial Y_c}{\partial B}</script><p>我们知道卷积层的计算公式为：</p>
<script type="math/tex; mode=display">
Y_c = X_c\cdot K_c+B</script><p>其中$B$为标量，对上式相对$B$求偏导，有：</p>
<script type="math/tex; mode=display">
\frac{\partial Y_c}{\partial B} = 1</script><p>带入第一个式子，可以得到$B$的梯度为：</p>
<script type="math/tex; mode=display">
\nabla B = \nabla Y_c</script><p>因此参数$B$的更新公式为：</p>
<script type="math/tex; mode=display">
B \leftarrow B - \eta \cdot \nabla Y_c</script><h4 id="1-2-1-2-权重K的梯度"><a href="#1-2-1-2-权重K的梯度" class="headerlink" title="1.2.1.2 权重K的梯度"></a>1.2.1.2 权重K的梯度</h4><p>由<code>1.1.2</code>小节介绍的矩阵公式，卷积层的前向过程为：</p>
<script type="math/tex; mode=display">
Y_c = X_c \cdot K_c + B</script><p>其中$K_c$为权重矩阵$K$进行矩阵变形产生的列向量：</p>
<script type="math/tex; mode=display">
K_c =
\left[
    \begin{matrix}
    k_{11} \\
  k_{12} \\
    k_{21} \\
  k_{22} \\
    \end{matrix}
\right
]</script><p>接着考虑权重$K_c$的梯度，由链式法则有：</p>
<script type="math/tex; mode=display">
\nabla K_c = \nabla Y_c \cdot \frac{\partial Y_c}{\partial K_c}</script><p>用矩阵求导公式，可以计算$\frac{\partial Y_c}{\partial K_c}$：</p>
<script type="math/tex; mode=display">
\frac{\partial Y_c}{\partial K_c}=X_c^T</script><p>$K_c$的梯度为：</p>
<script type="math/tex; mode=display">
\nabla K_c = \nabla Y_c \cdot X_c^T</script><p>因此$K_c$的更新公式为：</p>
<script type="math/tex; mode=display">
K_c \leftarrow K_c - \eta \cdot \nabla Y_c \cdot X_c^T</script><p>随后，将$K_c$变形回$K$即可。</p>
<h3 id="1-2-2-计算输入X的梯度"><a href="#1-2-2-计算输入X的梯度" class="headerlink" title="1.2.2 计算输入X的梯度"></a>1.2.2 计算输入X的梯度</h3><p>如果卷积层是第一层，那么只需要计算$K$和$B$的梯度就可以了。但如果卷积层是中间层，那么为了给上一层提供$\nabla Y$，还需要计算本层输入$X$的梯度$\nabla X$。</p>
<p>计算$\nabla X$的方法和<code>1.2.1</code>小节计算另外两个参数的方法有较大不同，这是因为$\frac{\partial X_c}{\partial X}$的计算十分复杂。</p>
<p>根据前向传播：</p>
<script type="math/tex; mode=display">
\left[
    \begin{matrix}
    y_{11}\\
    y_{12}\\
    y_{21}\\
    y_{22}\\
    \end{matrix}
\right]
=
\left[
    \begin{matrix}
    x_{11}k_{11} + x_{12}k_{12} + x_{21}k_{21} + x_{22}k_{22}\\
    x_{12}k_{12} + x_{13}k_{13} + x_{22}k_{22} + x_{23}k_{23}\\
    x_{21}k_{21} + x_{22}k_{22} + x_{31}k_{31} + x_{32}k_{32}\\
    x_{22}k_{22} + x_{23}k_{23} + x_{32}k_{32} + x_{33}k_{33}\\
    \end{matrix}
\right]</script><p>通过上式计算每个$x_{ij}$的梯度：</p>
<script type="math/tex; mode=display">
\begin{cases}
    \nabla x_{11} = \nabla y_{11} \cdot k_{11} \\
    \nabla x_{12} = \nabla y_{11} \cdot k_{12} + \nabla y_{12} \cdot k_{11} \\
    \nabla x_{13} = \nabla y_{12} \cdot k_{12} \\
    \nabla x_{21} = \nabla y_{21} \cdot k_{11} + \nabla y_{12} \cdot k_{21} \\
    \nabla x_{22} = \nabla y_{22} \cdot k_{11} + \nabla y_{21} \cdot k_{12}
          + \nabla y_{12} \cdot k_{21} + \nabla y_{11} \cdot k_{22} \\
    \nabla x_{23} = \nabla y_{22} \cdot k_{12}+ \nabla y_{12} \cdot k_{22}  \\
    \nabla x_{31} = \nabla y_{21} \cdot k_{21} \\
    \nabla x_{32} = \nabla y_{21} \cdot k_{22} + \nabla y_{22} \cdot k_{21}  \\
    \nabla x_{33} = \nabla y_{22} \cdot k_{22} \\
\end{cases}</script><p>整理成矩阵：</p>
<script type="math/tex; mode=display">
\nabla X_c = 
\left[
    \begin{matrix}
        \nabla x_{11}\\
        \nabla x_{12}\\
        \nabla x_{13}\\
        \nabla x_{21}\\
        \nabla x_{22}\\
        \nabla x_{23}\\
        \nabla x_{31}\\
        \nabla x_{32}\\
        \nabla x_{33}\\
    \end{matrix}
\right]
=
\left[
    \begin{matrix}
    0 & 0 & 0 & \nabla y_{11}\\
  0 & 0 & \nabla y_{11} & \nabla y_{12}\\
  0 & 0 & \nabla y_{12} & 0\\
  0 & \nabla y_{11} & 0 & \nabla y_{21}\\
  \nabla y_{11} & \nabla y_{12} & \nabla y_{21} & \nabla y_{22}\\
  \nabla y_{12} & 0 & \nabla y_{22} & 0\\
  0 & \nabla y_{21} & 0 & 0\\
  \nabla y_{21} & \nabla y_{22} & 0 & 0\\
  \nabla y_{22} & 0 & 0 & 0\\
    \end{matrix}
\right]

\cdot 

\left[
    \begin{matrix}
    k_{22} \\
  k_{21} \\
    k_{12} \\
  k_{11} \\
    \end{matrix}
\right]</script><p>其中，定义：</p>
<script type="math/tex; mode=display">
\nabla Y' 
=
\left[
    \begin{matrix}
    0 & 0 & 0 & \nabla y_{11}\\
  0 & 0 & \nabla y_{11} & \nabla y_{12}\\
  0 & 0 & \nabla y_{12} & 0\\
  0 & \nabla y_{11} & 0 & \nabla y_{21}\\
  \nabla y_{11} & \nabla y_{12} & \nabla y_{21} & \nabla y_{22}\\
  \nabla y_{12} & 0 & \nabla y_{22} & 0\\
  0 & \nabla y_{21} & 0 & 0\\
  \nabla y_{21} & \nabla y_{22} & 0 & 0\\
  \nabla y_{22} & 0 & 0 & 0\\
    \end{matrix}
\right]
,
\nabla K'
=
\left[
    \begin{matrix}
    k_{22} \\
  k_{21} \\
    k_{12} \\
  k_{11} \\
    \end{matrix}
\right]</script><p>注意到上述的$\nabla K’$是$\nabla K$的”转置列向量“。</p>
<p>上式可以反推回卷积公式，方法是把上面公式中$\nabla Y’$矩阵的每一行当成一个感受野，那么可以得到：</p>
<script type="math/tex; mode=display">
\nabla X = 
\left[
    \begin{matrix}
    \nabla x_{11} & \nabla x_{12} & \nabla x_{13}\\
    \nabla x_{21} & \nabla x_{22} & \nabla x_{23}\\
    \nabla x_{31} & \nabla x_{32} & \nabla x_{33}
    \end{matrix}
\right]
=
\left[
    \begin{matrix}
    0 & 0 & 0 & 0\\
    0 & \nabla y_{11} & \nabla y_{12} & 0\\
  0 & \nabla y_{21} & \nabla y_{22} & 0\\
  0 & 0 & 0 & 0
    \end{matrix}
\right]
*
\left[
    \begin{matrix}
    k_{22} & k_{21}\\
    k_{12} & k_{11}\\
    \end{matrix}
\right]</script><p>从上式知道，<strong>卷积层计算输入$X$的梯度，仍然是一个卷积计算的过程</strong>，可以归纳为：</p>
<script type="math/tex; mode=display">
\nabla X = \nabla Y_{p=1} * rot(K)</script><p>其中，$\nabla Y_{p=1}$是$\nabla Y$外加了一圈$zero\ padding$，$rot(K)$是$K$矩阵旋转180度得到的矩阵。</p>
<p>具体的计算过程为：</p>
<ol>
<li>把$\nabla Y$的四周进行$p=1$的$zero\ paddding$，得到$\nabla Y_{p=1}$，然后调用<code>1.1.2</code>小节提供的$img2col$方法转换成$\nabla Y’$</li>
<li>把$K$进行水平+竖直翻转，得到$rot(K)$，然后变形转换成$K’$</li>
<li>由$\nabla X_c = \nabla Y’ \cdot \nabla K’$计算$\nabla X_c$，然后变形转换成$\nabla X$</li>
</ol>
<p>这里计算得到的$\nabla X$会用于计算上一层网络的$\nabla Y$。</p>
<h1 id="2-转置卷积"><a href="#2-转置卷积" class="headerlink" title="2.转置卷积"></a>2.转置卷积</h1><p>参考资料：</p>
<p><a href="https://www.cnblogs.com/wmr95/p/9551490.html" target="_blank" rel="noopener">CSDN-转置卷积的理解</a></p>
<hr>
<p>转置卷积相对于卷积在神经网络结构的正向和反向传播中做相反的运算，如果说卷积层的功能是特征提取，转置卷积的作用就是从特征图中重构出图像。</p>
<p>转置卷积的通常用在三个领域：</p>
<ol>
<li><p>CNN可视化，通过反卷积将卷积得到的feature map还原到像素空间，来观察feature map对哪些pattern相应最大，即可视化哪些特征是卷积操作提取出来的；</p>
</li>
<li><p>FCN全卷积网络中，由于要对图像进行像素级的分割，需要将图像尺寸还原到原来的大小，类似upsampling的操作，所以需要采用反卷积；</p>
</li>
<li><p>GAN对抗式生成网络中，由于需要从输入图像到生成图像，自然需要将提取的特征图还原到和原图同样尺寸的大小，即也需要反卷积操作。</p>
</li>
</ol>
<h2 id="2-1卷积层性质回顾"><a href="#2-1卷积层性质回顾" class="headerlink" title="2.1卷积层性质回顾"></a>2.1卷积层性质回顾</h2><p>先来回顾以下上一章介绍的卷积的前向传播和反向传播。</p>
<p>前向传播：</p>
<script type="math/tex; mode=display">
Y=X * K + B \rightarrow Y_c = X_c \cdot K_c + B_c</script><p>反向传播：</p>
<script type="math/tex; mode=display">
\nabla X = \nabla Y_{p=1} * rot(K) \rightarrow \nabla X_c = \nabla Y' \cdot K'</script><p>在Tensorflow里，定义一个卷积层需要的参数有：</p>
<ul>
<li>卷积核大小$kernel_-size$</li>
<li>步长$strides$</li>
<li>填充数$padding$</li>
<li>输出通道数$filters$</li>
</ul>
<p>假设输入图像的尺寸为$(x_{in},x_{in})$，则输出特征图的尺寸为：</p>
<script type="math/tex; mode=display">
x_{out}=floor(x_{in}+2\times padding - \frac{kernel\ size}{strides})+1</script><h3 id="2-2-定义转置卷积"><a href="#2-2-定义转置卷积" class="headerlink" title="2.2 定义转置卷积"></a>2.2 定义转置卷积</h3><p>定义一个转置卷积需要的参数有：</p>
<ul>
<li>卷积核大小$kernel_-size$</li>
<li>步长$strides$</li>
<li>填充数$padding$</li>
<li>输出通道数$filters$</li>
</ul>
<p>逆卷积正向传播的过程分为两个阶段</p>
<ol>
<li><p>对输入的特征图$X$进行变换，得到新的特征图$X_{new}$</p>
<ol>
<li><p><strong>内部变换：当设置的$strides&gt;1$时，将对输入的特征图进行<code>0插值操作</code>(zero interpolation)。</strong></p>
<ul>
<li>具体操作方法就是在$X$的每行每列之间插入$strides-1$行和列的$0$，如下图是$strides=2,padding=1$时的示意图。</li>
<li>这是转置卷积和卷积唯一有差别的地方!<ul>
<li></li>
</ul>
</li>
</ul>
<p><img src="/2020/02/25/conv_deconv_upsample/strides=2.gif" alt="strides=2"></p>
</li>
<li><p>外部变换：如上图所示是$padding=1$的情况，卷积核的外边缘最多扫过原图外1像素处。该定义核卷积层一致。</p>
</li>
</ol>
</li>
<li><p>以实际步长=1，扫过$X_{new}$，同卷积一样，每个感受野计算得到一个像素。</p>
</li>
</ol>
<ul>
<li>值得注意的是，如果输入输出的通道数不一样，转置卷积层会在上述第一步前，用$[1\times1]$卷积核改变通道数。</li>
</ul>
<h2 id="2-3-实例理解转置卷积是卷积的逆运算"><a href="#2-3-实例理解转置卷积是卷积的逆运算" class="headerlink" title="2.3 实例理解转置卷积是卷积的逆运算"></a>2.3 实例理解转置卷积是卷积的逆运算</h2><p>举个简单的例子，如上节中的动图，现有输入图像$X$的尺寸为$[5,5]$，卷积核$K$尺寸为$[3,3]$，$strides=2$，$padding=1$。前向传播将$X$转化为$X_c$，其尺寸为$[9,9]$，前面的9是因为计算得到他有9个patch，后面的9是因为卷积核有9个参数，卷积核转化为$K_c$，其尺寸为$[9,1]$，输出$Y_c$的尺寸为$[9,1]$。</p>
<script type="math/tex; mode=display">
Y_c = X_c \cdot K_c :[9,1] =[9,9]\times[9,1]</script><p>现在来做上述卷积的逆运算，输入图像$X$的尺寸为$[3,3]$，卷积核$K$尺寸也为$[3,3]$。转置卷积的$X_c$尺寸为$[25,9]$，$K_c$为$[9,1]$,输出$Y_c$的尺寸为$[25,1]$。</p>
<script type="math/tex; mode=display">
Y_c = X_c \cdot K_c :[25,1] =[25,9]\times[9,1]</script><h1 id="3-上采样"><a href="#3-上采样" class="headerlink" title="3. 上采样"></a>3. 上采样</h1><p>使用GAN生成图像的时候，需要使用上采样算法增大特征图的尺寸。目前最常用的上采样算法有最邻近元插值法、双线性插值法，以及上一章提到的转置卷积法。</p>
<p>其中最邻近元插值法和双线性插值法属于不需训练的上采样方法，属于传统的内插值方法，输入和输出的图像看起来除尺寸有差别外差别不大，通道数也不变；而转置卷积则需要训练卷积核，通道数可以改变，输入图像和输出图像除了尺寸有差别之外，看起来差别很大。</p>
<p>在Tensorflow中，三者的API分别是：</p>
<ul>
<li>最邻近元插值法：<code>tf.keras.layers.Upsampling(size=(2,2),interpolation=&#39;nearest&#39;)</code></li>
<li>双线性插值法：<code>tf.keras.layers.Upsampling(size=(2,2),interpolation=&#39;bilinear&#39;)</code></li>
<li>转置卷积：<code>tf.keras.layers.ConvTranspose2D(filters,kernel_size,padding=&#39;valid&#39;)</code></li>
</ul>
<p>下面介绍前两种方法，以及针对的转置卷积法与非训练方法的优劣做一点补充。</p>
<h2 id="3-1常用上采样方法"><a href="#3-1常用上采样方法" class="headerlink" title="3.1常用上采样方法"></a>3.1常用上采样方法</h2><h3 id="3-1-1-最邻近元插值-nearest"><a href="#3-1-1-最邻近元插值-nearest" class="headerlink" title="3.1.1 最邻近元插值(nearest)"></a>3.1.1 最邻近元插值(nearest)</h3><p>这是最简单的一种插值方法，不需要计算，在待求像素的上下左右四邻像素中，将距离待求像素最近的邻想素灰度赋给待求象素。</p>
<p>通过下面这张图可以秒懂。</p>
<p><img src="/2020/02/25/conv_deconv_upsample/nearest.png" alt="nearest"></p>
<h3 id="3-1-2-双线性插值-bilinear"><a href="#3-1-2-双线性插值-bilinear" class="headerlink" title="3.1.2 双线性插值(bilinear)"></a>3.1.2 双线性插值(bilinear)</h3><p><strong>双线性插值</strong>，又称为<strong>双线性内插</strong>。在数学上，<strong>双线性插值</strong>是对线性插值在二维直角网格上的扩展，用于对双变量函数（例如 $x$和$y$）进行插值。其核心思想是在两个方向分别进行一次线性插值。</p>
<p>假设我们想得到未知函数$f$在点$P=(x, y)$的值，假设我们已知函数$f$在四个组成矩形的点$Q_{11}= (x_1, y_1)$、$Q_{12}=(x_1, y_2)$,$Q_{21}= (x_2, y_1)$以及 $Q_{22} = (x_2, y_2)$上的值。</p>
<p>首先在 $x$方向进行线性插值，分别连接$(Q_{11},Q_{12})$和$(Q_{21},Q_{22})$，拟合出两条直线:</p>
<script type="math/tex; mode=display">
f(x,y_1) \approx \frac{x_2-x}{x_2-x_1}f(x_1,y_1)+\frac{x-x_1}{x_2-x_1}f(x_2,y_1)\\
f(x,y_2) \approx \frac{x_2-x}{x_2-x_1}f(x_1,y_2)+\frac{x-x_1}{x_2-x_1}f(x_2,y_2)</script><p>假设要求的是点$P=(x_3,y_3)$上的值，那么先用上述公式，计算出$R_1=(x_3,y_1)$$R_2=(x_3,y_2)$上的值，然后再将$R_1$和$R_2$连成一条直线，求该直线在$(x_3,y_3)$点的值即可。</p>
<p>搭配下面这张图应该也可以秒懂。</p>
<p><img src="/2020/02/25/conv_deconv_upsample/bilinear.png" alt="bilinear"></p>
<h2 id="3-2-上采样与转置卷积的优劣"><a href="#3-2-上采样与转置卷积的优劣" class="headerlink" title="3.2 上采样与转置卷积的优劣"></a>3.2 上采样与转置卷积的优劣</h2><p>最邻近元插值法效果最差，生成图像有锯齿形，但是占用的显存最少，是tensorflow默认的插值方法。</p>
<p>双线性插值法效果最好，生成的图像很平滑，但是占用的显存最多。</p>
<p>转置卷积的效果不错，但是参数配置失误会出现棋盘状的噪点，因为只有在kernel/strides可以整除的时候才是均匀采样。<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">有一篇文章</a>专门介绍了这个问题以及如何避免它。占用的显存相对于双线性插值稍微少一点。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CV/" rel="tag"># CV</a>
              <a href="/tags/MachineLearning/" rel="tag"># MachineLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/19/gan-02/" rel="prev" title="GAN[02]-边界均衡生成对抗网络:BEGAN">
      <i class="fa fa-chevron-left"></i> GAN[02]-边界均衡生成对抗网络:BEGAN
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/28/improve_network/" rel="next" title="改善深层神经网络的方法：消除过拟合、加速训练与超参数调试">
      改善深层神经网络的方法：消除过拟合、加速训练与超参数调试 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-摘要"><span class="nav-text">0.摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-卷积"><span class="nav-text">1.卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1卷积层的正向传播"><span class="nav-text">1.1卷积层的正向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-常规思路理解卷积"><span class="nav-text">1.1.1 常规思路理解卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-用矩阵乘法计算卷积（im2col法实现）"><span class="nav-text">1.1.2 用矩阵乘法计算卷积（im2col法实现）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-卷积层的反向传播"><span class="nav-text">1.2 卷积层的反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-计算权重-K-和偏置-B-的梯度"><span class="nav-text">1.2.1 计算权重$K$和偏置$B$的梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-1-偏置B的梯度"><span class="nav-text">1.2.1.1 偏置B的梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-2-权重K的梯度"><span class="nav-text">1.2.1.2 权重K的梯度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-计算输入X的梯度"><span class="nav-text">1.2.2 计算输入X的梯度</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-转置卷积"><span class="nav-text">2.转置卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1卷积层性质回顾"><span class="nav-text">2.1卷积层性质回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-定义转置卷积"><span class="nav-text">2.2 定义转置卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-实例理解转置卷积是卷积的逆运算"><span class="nav-text">2.3 实例理解转置卷积是卷积的逆运算</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-上采样"><span class="nav-text">3. 上采样</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1常用上采样方法"><span class="nav-text">3.1常用上采样方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-最邻近元插值-nearest"><span class="nav-text">3.1.1 最邻近元插值(nearest)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-双线性插值-bilinear"><span class="nav-text">3.1.2 双线性插值(bilinear)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-上采样与转置卷积的优劣"><span class="nav-text">3.2 上采样与转置卷积的优劣</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhenhui Ye"
      src="/images/face.jpg">
  <p class="site-author-name" itemprop="name">Zhenhui Ye</p>
  <div class="site-description" itemprop="description">Be a superhero.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yerfor" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yerfor" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhenhuiye@zju.edu.com" title="E-Mail → mailto:zhenhuiye@zju.edu.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhenhui Ye</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.7.1
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
