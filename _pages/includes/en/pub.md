# üìù Publications


## ü¶∏ Digital Avatar

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024 Spotlight</div><img src='../images/real3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MimicTalk: Mimicking a personalized and expressive 3D talking face in few minutes](https://openreview.net/forum?id=gjEzL0bamb) \\
**Zhenhui Ye**, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei Huang, Rongjie Huang, Jinglin Liu, Chen Zhang, Zehan Wang, Xize Chen, Xiang Yin, Zhou Zhao

**NeurIPS 2024** 

[**Project Page**](https://mimictalk.github.io/) [![img](https://img.shields.io/github/stars/yerfor/MimicTalk?style=social)](https://github.com/yerfor/MimicTalk)

- MimicTalk aims at training a high-quality personalized digital avatar in several minutes.
- By ICS-A2M Model, we can in-context mimicking the talking style of the target ID.
- By fine-tuning Real3D-Portrait (Our previous large-scale talking face model), we can achieve fast adaptation in several minutes.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024 Spotlight</div><img src='../images/real3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis](https://openreview.net/forum?id=7ERQPyR2eb) \\
**Zhenhui Ye**, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun MA, Zhou Zhao

**ICLR 2024 Spotlight** 

[**Project Page**](https://real3dportrait.github.io/) [![img](https://img.shields.io/github/stars/yerfor/Real3DPortrait?style=social)](https://github.com/yerfor/Real3DPortrait)

- Real3D-Portrait is the first one-shot NeRF-based talking face system with realistic head, torso, and background segments.
- It facilitates both audio / video-driven one-shot talking face generation.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='../images/geneface++.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Geneface++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation](https://arxiv.org/abs/2305.00787) \\
**Zhenhui Ye**, Jinzheng He, Ziyue Jiang, Rongjie Huang, Jiangwei Huang, Jinglin Liu, Yi Ren, Xiang Yin, Zejun Ma, Zhou Zhao

**Under Review** 

[**Project Page**](https://genefaceplusplus.github.io)) [![img](https://img.shields.io/github/stars/yerfor/GeneFacePlusPlus?style=social)](https://github.com/yerfor/GeneFacePlusPlus)

- GeneFace++ is a modern talking face system that aims to achieve the goal of generalized lip synchronization, good video quality, and high system efficiency.
- It greatly improves the stability and efficiency of NeRF-based methods.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='../images/geneface.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Geneface: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis](https://arxiv.org/abs/2301.13430) \\
**Zhenhui Ye**, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He, Zhou Zhao

**ICLR 2023 Poster** 

[**Project Page**](https://geneface.github.io) [![img](https://img.shields.io/github/stars/yerfor/GeneFace?style=social)](https://github.com/yerfor/GeneFace)

- GeneFace is a NeRF-based talking face system that generalizes well to various OOD audios.
- It first utilizes a generative model to model the audio-to-motion mapping.

</div>
</div>


## üéô Speech Synthesis


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2023</div><img src='../images/clapspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training](https://arxiv.org/abs/2305.10763) \\
**Zhenhui Ye**, Rongjie Huang, Yi Ren, Ziyue Jiang, Jinglin Liu, Jinzheng He, Zhou Zhao

**ACL 2023 Poster** 

[**Project Page**](https://clapspeech.github.io) [![img](https://img.shields.io/github/stars/yerfor/CLAPSpeech?style=social)](https://github.com/yerfor/CLAPSpeech)

- CLAPSpeech is the first cross-modal contrastive learning method that focus on extracting prosody-related text representation for text-to-speech (TTS).
- It provides a convenient plug-in text encoder applicable for all TTS models to improve prosody.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2022</div><img src='../images/synta.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech](https://arxiv.org/abs/2204.11792) \\
**Zhenhui Ye**, Zhou Zhao, Yi Ren, Fei Wu

**IJCAI 2022 Poster** 

[**Project Page**](https://syntaspeech.github.io) [![img](https://img.shields.io/github/stars/yerfor/SyntaSpeech?style=social)](https://github.com/yerfor/SyntaSpeech)

- SyntaSpeech is the first syntax-aware non-autoregressive TTS acoustic model.
- We design a syntatic graph encoder to extract syntax-related prosody from text.

</div>
</div>

- [Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias](https://arxiv.org/pdf/2306.03509v1.pdf), Ziyue Jiang, Yi Ren, **Zhenhui Ye**, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao, ICLR 2024


## üìö Deep Reinforcement Learning

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TMC 2022</div><img src='../images/drgn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Soft-DRGN: Multi-UAV Navigation for Partially Observable Communication Coverage by Graph Reinforcement Learning](https://ieeexplore.ieee.org/document/9697395) \\
**Zhenhui Ye**, Ke Wang, Yining Chen, Xiaohong Jiang, Guanghua Song. 

**IEEE transactions on Mobile Computing 2022** 

[**Project Page**](https://github.com/yerfor/Soft-DRGN) [![img](https://img.shields.io/github/stars/yerfor/Soft-DRGN?style=social)](https://github.com/yerfor/Soft-DRGN)
 
- We propose Soft-DRGN to learn robust stochastic policies for large-scale multi-agent cooperation.
- We propose to utilize graph attention network to learn the inter-agent communication.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Applied Intelligence 2022</div><img src='../images/experience_augmentation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Improving Sample Efficiency in Multi-Agent Actor-Critic Methods](https://link.springer.com/article/10.1007/s10489-021-02554-5)

**Zhenhui Ye**, Yining Chen, Xiaohong Jiang, Guanghua Song, 

**Applied Intelligence 2022**

- We propose Experience Augmentation (EA) to improve the sample efficiency for homogeneous MARL tasks.
- We propose a sample-efficient training pipeline called PEDMA.

</div>
</div>

- [Multi-agent Deep Reinforcement Learning for Voltage Control with Coordinated Active and Reactive Power Optimization](https://ieeexplore.ieee.org/document/9805763), Daner Hu, **Zhenhui Ye**, Yuanqi Gao, Zuzhao Ye, Yonggang Peng, Napeng Yu, **IEEE transactions on Smart Grid 2022**
